{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import math\n",
    "import geopandas as gpd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    cluster   id\n",
      "0         1  131\n",
      "1         1  132\n",
      "2         1  133\n",
      "3         1  134\n",
      "4         1  162\n",
      "5         1  163\n",
      "6         1  164\n",
      "7         1  165\n",
      "8         1  166\n",
      "9         1  167\n",
      "10        1  168\n",
      "11        1  169\n",
      "12        1  170\n",
      "13        1  171\n",
      "14        1  172\n",
      "15        1  173\n",
      "16        1  182\n",
      "17        1  183\n",
      "18        1  184\n",
      "19        1  185\n",
      "20        1  186\n",
      "21        1  187\n",
      "22        1  188\n",
      "23        1  189\n",
      "24        1  190\n"
     ]
    }
   ],
   "source": [
    "od_data = np.load('od_matrix.npy')\n",
    "duration_data = np.load('duration_matrix.npy')\n",
    "distance_data = np.load('distance_matrix.npy')\n",
    "# remove transit zones\n",
    "od_data = od_data[:-3, :-3, :]\n",
    "duration_data = duration_data[:-3, :-3, :]\n",
    "distance_data = distance_data[:-3, :-3, :]\n",
    "# set remaing nans to avg\n",
    "avg_duration = int(np.nanmean(duration_data))\n",
    "duration_data[np.isnan(duration_data)] = avg_duration\n",
    "avg_distance = int(np.nanmean(distance_data))\n",
    "distance_data[np.isnan(distance_data)] = avg_distance\n",
    "\n",
    "# 5 clusters\n",
    "parts = np.zeros(190, dtype=int) - 1\n",
    "for cluster in range(1,6):\n",
    "    shapefile_path = f\"./5_zones/{cluster}.shp\"\n",
    "    df = gpd.read_file(shapefile_path)\n",
    "    for df_row in range(df.shape[0]):\n",
    "        parts[df[\"id\"][df_row]-1] = cluster-1\n",
    "# print(parts)\n",
    "parts_5 = np.delete(parts, 41)\n",
    "print(parts_5)\n",
    "print(parts_5.shape)\n",
    "\n",
    "# 10 clusters\n",
    "parts = np.zeros(190, dtype=int) - 1\n",
    "for cluster in range(1,11):\n",
    "    shapefile_path = f\"./10_zones/{cluster}.shp\"\n",
    "    df = gpd.read_file(shapefile_path)\n",
    "    for df_row in range(df.shape[0]):\n",
    "        parts[df[\"id\"][df_row]-1] = cluster-1\n",
    "# print(parts)\n",
    "parts_10 = np.delete(parts, 41)\n",
    "print(parts_10)\n",
    "print(parts_10.shape)\n",
    "\n",
    "# 20 clusters\n",
    "parts = np.zeros(190, dtype=int) - 1\n",
    "for cluster in range(1,21):\n",
    "    shapefile_path = f\"./20_zones/{cluster}.shp\"\n",
    "    df = gpd.read_file(shapefile_path)\n",
    "    for df_row in range(df.shape[0]):\n",
    "        parts[df[\"id\"][df_row]-1] = cluster-1\n",
    "# print(parts)\n",
    "parts_20 = np.delete(parts, 41)\n",
    "print(parts_20)\n",
    "print(parts_20.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_clustered_matrix(parts, number_clusters, number_of_transit_zones, matrix_type, data_path: str):\n",
    "    data = np.load(data_path)\n",
    "    data = data[:-3, :-3, :]\n",
    "    time = data.shape[2]\n",
    "    clustered_matrix = np.zeros((number_clusters+number_of_transit_zones, number_clusters+number_of_transit_zones, time), dtype=int)\n",
    "    normalization_matrix = np.zeros((number_clusters+number_of_transit_zones, number_clusters+number_of_transit_zones, time))\n",
    "    \n",
    "    for origin_area in range(data.shape[0]):\n",
    "        if origin_area == 41:\n",
    "            continue\n",
    "        if origin_area > 41:\n",
    "            parts_origin = origin_area-1\n",
    "        else:\n",
    "            parts_origin = origin_area\n",
    "        for destination_area in range(data.shape[1]):\n",
    "            if destination_area == 41:\n",
    "                continue\n",
    "            if destination_area > 41:\n",
    "                parts_dest = destination_area-1\n",
    "            else:\n",
    "                parts_dest = destination_area\n",
    "            for hour in range(time):\n",
    "                if  not np.isnan(data[origin_area, destination_area, hour]):\n",
    "                    clustered_matrix[parts[parts_origin], parts[parts_dest], hour] += data[origin_area, destination_area, hour]\n",
    "                    normalization_matrix[parts[parts_origin], parts[parts_dest], hour] += 1\n",
    "    \n",
    "    if matrix_type == 'duration' or matrix_type == 'distance':\n",
    "        clustered_matrix = (clustered_matrix/normalization_matrix).round(1)\n",
    "    # print(clustered_matrix)\n",
    "    np.save(str(number_clusters)+'_clusters/' + data_path, clustered_matrix)\n",
    "    return clustered_matrix\n",
    "\n",
    "# create corresponding od matrix\n",
    "number_clusters = 5\n",
    "number_of_transit_zones = 0\n",
    "clustered_od_matrix = get_clustered_matrix(parts_5, number_clusters, number_of_transit_zones,'od', 'od_matrix.npy')\n",
    "clustered_od_matrix = get_clustered_matrix(parts_5, number_clusters, number_of_transit_zones,'duration', 'duration_matrix.npy')\n",
    "clustered_od_matrix = get_clustered_matrix(parts_5, number_clusters, number_of_transit_zones,'distance', 'distance_matrix.npy')\n",
    "\n",
    "# create corresponding od matrix\n",
    "number_clusters = 10\n",
    "number_of_transit_zones = 0\n",
    "clustered_od_matrix = get_clustered_matrix(parts_10, number_clusters, number_of_transit_zones,'od', 'od_matrix.npy')\n",
    "clustered_od_matrix = get_clustered_matrix(parts_10, number_clusters, number_of_transit_zones,'duration', 'duration_matrix.npy')\n",
    "clustered_od_matrix = get_clustered_matrix(parts_10, number_clusters, number_of_transit_zones,'distance', 'distance_matrix.npy')\n",
    "\n",
    "# create corresponding od matrix\n",
    "number_clusters = 20\n",
    "number_of_transit_zones = 0\n",
    "clustered_od_matrix = get_clustered_matrix(parts_20, number_clusters, number_of_transit_zones,'od', 'od_matrix.npy')\n",
    "clustered_od_matrix = get_clustered_matrix(parts_20, number_clusters, number_of_transit_zones,'duration', 'duration_matrix.npy')\n",
    "clustered_od_matrix = get_clustered_matrix(parts_20, number_clusters, number_of_transit_zones,'distance', 'distance_matrix.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 5, 24)\n",
      "1743156\n"
     ]
    }
   ],
   "source": [
    "od_data = np.load('5_clusters/od_matrix.npy')\n",
    "print(od_data.shape)\n",
    "print(od_data[:,:,8:-4].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6008.162569728143\n",
      "5946\n",
      "(20,)\n",
      "5906.0\n",
      "[372. 692. 314.  44. 252. 288. 344. 400. 267. 454.  52. 160.  52. 254.\n",
      " 282.  68. 225. 219. 691. 476.]\n"
     ]
    }
   ],
   "source": [
    "# find charge station distribution over TAZs using numpy output\n",
    "# only interested on 50kW -> third output, using this for RL approach\n",
    "\n",
    "existing_charger_distr_multiple_different_kW = np.load(\"UMax_charge_Justins_optim.npy\")\n",
    "existing_charger_distr = existing_charger_distr_multiple_different_kW[:,2] # only 50 kW\n",
    "print(existing_charger_distr.sum())\n",
    "\n",
    "map_existing_charger_clusters_to_taz =  {\n",
    "    1: [56, 57],\n",
    "    2: [52, 62, 65, 66],\n",
    "    3: [43, 48, 49, 50, 51, 70, 71, 72],\n",
    "    4: [6, 7, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 39, 40, 41, 44, 45, 46, 47, 73, 74, 75],\n",
    "    5: [1, 2, 3, 4, 5, 12, 13, 14, 15, 16, 17, 22, 23, 24, 25, 26, 37, 38, 42],\n",
    "    6: [54, 55, 58, 59, 60],\n",
    "    7: [53, 61, 63, 64, 67, 90],\n",
    "    8: [68, 69, 83, 84, 85, 86, 87, 88, 89],\n",
    "    9: [8, 9, 10, 11, 20, 76, 77, 78, 79, 80, 81, 82, 104, 105, 106, 107],\n",
    "    10: [18, 19, 21, 108, 109, 110],\n",
    "    11: [178, 179, 180, 181, 184],\n",
    "    12: [172, 173, 174, 175, 176, 177, 185],\n",
    "    13: [91, 92, 93, 94, 95, 96, 129, 171],\n",
    "    14: [97, 98, 99, 100, 101, 102, 103, 116, 117, 118, 119, 122, 123, 128],\n",
    "    15: [111, 112, 113, 114, 115, 120, 121, 142],\n",
    "    16: [182, 183, 186, 187],\n",
    "    17: [169, 170, 188],\n",
    "    18: [130, 131, 132, 133, 134],\n",
    "    19: [124, 125, 126, 127, 135, 136, 137, 138, 152],\n",
    "    20: [139, 140, 141, 143, 144, 145, 146, 147, 150],\n",
    "    21: [190],\n",
    "    22: [168, 189],\n",
    "    23: [161, 162, 163, 164, 165, 166, 167],\n",
    "    24: [155, 156, 157, 158, 159, 160],\n",
    "    25: [148, 149, 151, 153, 154]\n",
    "    }\n",
    "\n",
    "number_baseline_clusters = 25\n",
    "# build global charger dictionary\n",
    "taz_chargers = []\n",
    "number_taz = 190\n",
    "for area in range(number_baseline_clusters):\n",
    "    taz_in_area = map_existing_charger_clusters_to_taz[area+1]\n",
    "    number_taz_in_area = len(taz_in_area)\n",
    "    number_chargers = int(existing_charger_distr[area]/number_taz_in_area)\n",
    "    for taz in taz_in_area:\n",
    "        taz_chargers.append(number_chargers)\n",
    "print(sum(taz_chargers))\n",
    "\n",
    "# create chargefiles for tasks\n",
    "# top-level\n",
    "taz_to_clusters = parts_20 # remove values corresponding to boundary zones\n",
    "number_clusters = 20\n",
    "top_level_chargers = np.zeros(number_clusters)\n",
    "for taz in range(number_taz):\n",
    "    if taz == 41:\n",
    "        continue\n",
    "    if taz > 41:\n",
    "        taz -= 1\n",
    "    cluster = taz_to_clusters[taz] \n",
    "    top_level_chargers[cluster] += taz_chargers[taz]\n",
    "print(top_level_chargers.shape)\n",
    "print(top_level_chargers.sum())\n",
    "print(top_level_chargers)\n",
    "np.save(str(number_clusters)+'_clusters/charging_stations.npy', top_level_chargers)\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dominikhelmreich/opt/anaconda3/envs/pamod/lib/python3.7/site-packages/geopandas/array.py:85: ShapelyDeprecationWarning: __len__ for multi-part geometries is deprecated and will be removed in Shapely 2.0. Check the length of the `geoms` property instead to get the  number of parts of a multi-part geometry.\n",
      "  aout[:] = out\n",
      "/Users/dominikhelmreich/opt/anaconda3/envs/pamod/lib/python3.7/site-packages/geopandas/plotting.py:38: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  for poly in geom:\n",
      "/Users/dominikhelmreich/opt/anaconda3/envs/pamod/lib/python3.7/site-packages/descartes/patch.py:65: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  for t in polygon])\n",
      "/Users/dominikhelmreich/opt/anaconda3/envs/pamod/lib/python3.7/site-packages/geopandas/plotting.py:38: ShapelyDeprecationWarning: Iteration over multi-part geometries is deprecated and will be removed in Shapely 2.0. Use the `geoms` property to access the constituent parts of a multi-part geometry.\n",
      "  for poly in geom:\n",
      "/Users/dominikhelmreich/opt/anaconda3/envs/pamod/lib/python3.7/site-packages/descartes/patch.py:65: ShapelyDeprecationWarning: The array interface is deprecated and will no longer work in Shapely 2.0. Convert the '.coords' to a numpy array instead.\n",
      "  for t in polygon])\n"
     ]
    }
   ],
   "source": [
    "# plotting clusters:\n",
    "from geopandas.plotting import plot_polygon_collection\n",
    "\n",
    "number_clusters = 5\n",
    "shapefile_path = f\"./{number_clusters}_zones/1.shp\"\n",
    "city = gpd.read_file(shapefile_path)\n",
    "city[\"cluster\"] = 1\n",
    "for cluster in range(2,number_clusters+1):\n",
    "    shapefile_path = f\"./{number_clusters}_zones/{cluster}.shp\"\n",
    "    df = gpd.read_file(shapefile_path)\n",
    "    df[\"cluster\"] = cluster\n",
    "    city = city.append(df)\n",
    "# print(city)\n",
    "fig, ax = plt.subplots(figsize=(13, 6.5))\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_visible(False)\n",
    "ax.spines['left'].set_visible(False)\n",
    "y_ticks = [i for i in range(number_clusters+1)]\n",
    "city.plot(ax=ax, column='cluster', vmax=number_clusters, legend=True, legend_kwds={'ticks': y_ticks})\n",
    "plot_polygon_collection(ax, city['geometry'], vmax=number_clusters, values=city['cluster'])\n",
    "# plt.xlim((-122.525, -122.35))\n",
    "# plt.ylim((37.7, 37.850))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.savefig(str(number_clusters)+'_clusters/cluster_plot.pdf')\n",
    "# plt.show()\n",
    "plt.close()\n",
    "\n",
    "# number_clusters = 10\n",
    "# shapefile_path = f\"./{number_clusters}_zones/1.shp\"\n",
    "# city = gpd.read_file(shapefile_path)\n",
    "# city[\"cluster\"] = 1\n",
    "# for cluster in range(2,number_clusters+1):\n",
    "#     shapefile_path = f\"./{number_clusters}_zones/{cluster}.shp\"\n",
    "#     df = gpd.read_file(shapefile_path)\n",
    "#     df[\"cluster\"] = cluster\n",
    "#     city = city.append(df)\n",
    "# # print(city)\n",
    "# fig, ax = plt.subplots(figsize=(13, 6.5))\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['bottom'].set_visible(False)\n",
    "# ax.spines['left'].set_visible(False)\n",
    "# y_ticks = [i for i in range(number_clusters+1)]\n",
    "# city.plot(ax=ax, column='cluster', vmax=number_clusters, legend=True, cmap=plt.get_cmap(\"tab10\"), legend_kwds={'ticks': y_ticks})\n",
    "# plot_polygon_collection(ax, city['geometry'], vmax=number_clusters, values=city['cluster'], cmap=plt.get_cmap(\"tab10\"))\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "# plt.savefig(str(number_clusters)+'_clusters/cluster_plot.pdf')\n",
    "# # plt.show()\n",
    "# plt.close()\n",
    "\n",
    "# number_clusters = 20\n",
    "# shapefile_path = f\"./{number_clusters}_zones/1.shp\"\n",
    "# city = gpd.read_file(shapefile_path)\n",
    "# city[\"cluster\"] = 1\n",
    "# for cluster in range(2,number_clusters+1):\n",
    "#     shapefile_path = f\"./{number_clusters}_zones/{cluster}.shp\"\n",
    "#     df = gpd.read_file(shapefile_path)\n",
    "#     df[\"cluster\"] = cluster\n",
    "#     city = city.append(df)\n",
    "# # print(city)\n",
    "# fig, ax = plt.subplots(figsize=(13, 6.5))\n",
    "# ax.spines['top'].set_visible(False)\n",
    "# ax.spines['right'].set_visible(False)\n",
    "# ax.spines['bottom'].set_visible(False)\n",
    "# ax.spines['left'].set_visible(False)\n",
    "# y_ticks = [i for i in range(number_clusters+1)]\n",
    "# city.plot(ax=ax, column='cluster', vmax=number_clusters, legend=True, cmap=plt.get_cmap(\"tab20\"), legend_kwds={'ticks': y_ticks})\n",
    "# plot_polygon_collection(ax, city['geometry'], vmax=number_clusters, values=city['cluster'], cmap=plt.get_cmap(\"tab20\"))\n",
    "# plt.xticks([])\n",
    "# plt.yticks([])\n",
    "# plt.savefig(str(number_clusters)+'_clusters/cluster_plot.pdf')\n",
    "# # plt.show()\n",
    "# plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('pamod')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0f8edb555df32b3dff9504d8c4d29eadaf231ea869cc9ae1afb4d8556d0e264c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
